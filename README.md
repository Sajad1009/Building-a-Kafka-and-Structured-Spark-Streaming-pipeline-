Large number of companies across a multitude of industries are currently maintaining data pipelines used to ingest and analyze
a huge data streams. In effect, the proper implementation of such pipelines belongs to the realm of “data engineering”,
and represents a gateway to interesting data science-related problems. Traditional machine learning methods have been developed to work
using batch or offline approaches,but there are fewer options when we start considering solutions for true streaming problems.

Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live
data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex
algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.
In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.

